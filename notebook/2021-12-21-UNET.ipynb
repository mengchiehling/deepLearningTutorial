{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 區域分割 （工具）\n",
    "2. 區域 -> 類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNET model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "input_size = (1024, 1024, 3)\n",
    "\n",
    "def unet(input_size):\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)   # 64: filters, 3: kernel size\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=2)(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)   # 64: filters, 3: kernel size\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=2)(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)   # 64: filters, 3: kernel size\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=2)(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)   # 64: filters, 3: kernel size\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=2)(conv4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)   # 64: filters, 3: kernel size\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    # up-scaling\n",
    "\n",
    "    up6 = UpSampling2D(size=2)(conv5)\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same')(up6)\n",
    "    merge6 = concatenate([conv4 , up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    # output_shape = (128, 128, 512)\n",
    "\n",
    "    up7 = UpSampling2D(size=2)(conv6)\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same')(up7)\n",
    "    merge7 = concatenate([conv3 , up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    # output_shape = (256, 256, 256)\n",
    "\n",
    "    up8 = UpSampling2D(size=2)(conv7)\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same')(up8)\n",
    "    merge8 = concatenate([conv2 , up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    # output_shape = (512, 512, 128)\n",
    "\n",
    "    up9 = UpSampling2D(size=2)(conv8)\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same')(up9)\n",
    "    merge9 = concatenate([conv1 , up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    # output_shape = (1024, 1024, 128)\n",
    "\n",
    "    outputs = Conv2D(1, 3, activation='sigmoid', padding='same')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(input_size=input_size)\n",
    "model.compile(optimizer = Adam(lr = 1e-4), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "mask = np.array(Image.open(\"../data/train/104_mask.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob(f\"../data/train/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_files = [f.replace('sat', 'mask').replace('jpg', 'png') for f in data_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(data=np.array([data_files, mask_files]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, dir: str, img_col: str, mask_col: str, img_augmentation, sample_size: Optional[int] = None, batch_size: int = 32, shuffle: bool = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dir: directory in which images are stored\n",
    "            sample_size: Optional; number of images will be sampled in each of sub_directory,\n",
    "            from tying import Union\n",
    "            sample_size: Union[int, None] -> Optional[int]\n",
    "            if not provided all images in the dir are taken into account.\n",
    "            batch_size: number of images in each of batch\n",
    "            shuffle: if shuffle the order of the data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dir = dir\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_size = sample_size\n",
    "        self.img_col = img_col\n",
    "        self.mask_col = mask_col\n",
    "        self.img_augmentation = img_augmentation\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        self.max = self.__len__()\n",
    "        self.n = 0\n",
    "\n",
    "    def __transform_to_dataframe(self) -> pd.DataFrame:\n",
    "        \n",
    "        \"\"\"\n",
    "        transform the data into a pandas dataframe to track the image files and the corresponding masks\n",
    "        \"\"\"\n",
    "        \n",
    "        dir_ = f\"../data/{self.dir}\"\n",
    "\n",
    "        data = []\n",
    "\n",
    "        data_files = glob(f\"{dir_}/*.jpg\")\n",
    "        \n",
    "        if self.sample_size:\n",
    "            sampled_files = random.sample(data_files, min(self.sample_size, len(data_files)))\n",
    "        else:\n",
    "            sampled_files = data_files\n",
    "        \n",
    "        mask_files = [f.replace('sat', 'mask').replace('jpg', 'png') for f in sampled_files]\n",
    "    \n",
    "        df = pd.DataFrame(data=np.array([data_files, mask_files]).T, columns=[self.img_col, self.mask_col], dtype=object)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.df = self.__transform_to_dataframe()\n",
    "        self.indices = self.df.index.tolist()\n",
    "\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __len__(self):\n",
    "        #  Denotes the number of batches per epoch\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # Generate one batch of data\n",
    "        # Generate indices of the batch\n",
    "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # Find list of IDs\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        # Generate data\n",
    "        X, y = self.__get_data(batch)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __get_data(self, batch: List) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        df_batch = self.df.loc[batch]\n",
    "\n",
    "        sat_dataset = []\n",
    "        mask_dataset = []\n",
    "\n",
    "        for _, row in df_batch.iterrows():\n",
    "            # lock the image augmentation\n",
    "            seq_det = self.img_augmentation.to_deterministic()\n",
    "            \n",
    "            # input image\n",
    "            f = row[self.img_col]\n",
    "            sat_image = seq_det.augment_image(np.array(Image.open(f)))\n",
    "            sat_dataset.append(sat_image/255.0)\n",
    "            \n",
    "            \n",
    "            # mask image\n",
    "            f = row[self.mask_col]\n",
    "            mask_image = seq_det.augment_image(np.array(Image.open(f).convert('L')), hooks=ia.HooksImages(activator=self.activator))\n",
    "            mask_dataset.append(mask_image//255)\n",
    "            \n",
    "            # for multiclasses: one-hot encoding\n",
    "            # new_mask = np.zeros(mask.shape + (num_classes, ))\n",
    "            # for i in range(num_classes):\n",
    "            #   new_mask[mask==i, i] = 1\n",
    "\n",
    "        return np.array(sat_dataset), np.array(mask_dataset)\n",
    "\n",
    "    def __next__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        generate data of size batch_size\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.n >= self.max:\n",
    "            self.n = 0\n",
    "\n",
    "        result = self.__getitem__(self.n)\n",
    "        self.n += 1\n",
    "        return result\n",
    "    \n",
    "    def activator(self, images, augmenter, parents, default):\n",
    "        return False if augmenter.name in [\"GaussianBlur\"] else default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let us see how image augmentation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "    iaa.Flipud(0.5),\n",
    "    iaa.GaussianBlur(sigma=(0, 3.0)) # blur images with a sigma of 0 to 3.0\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_image = np.array(Image.open(\"../data/train/104_sat.jpg\"))\n",
    "mask_image = np.array(Image.open(\"../data/train/104_mask.png\").convert('L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_det_1 = seq.to_deterministic()\n",
    "sat_image_1 = seq_det_1.augment_image(sat_image)\n",
    "mask_image_1 = seq_det_1.augment_image(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sat_image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_det_2 = seq.to_deterministic()\n",
    "sat_image_2 = seq_det_2.augment_image(sat_image)\n",
    "mask_image_2 = seq_det_2.augment_image(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sat_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activator(images, augmenter, parents, default):\n",
    "    return False if augmenter.name in [\"GaussianBlur\"] else default\n",
    "\n",
    "seq_det_2.augment_image(mask_image, hooks=ia.HooksImages(activator=activator)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "datagen = DataGenerator(dir='train', img_col='sat', mask_col='mask', img_augmentation=seq, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = next(datagen)  # sat image, mask image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=datagen, steps_per_epoch=4, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
